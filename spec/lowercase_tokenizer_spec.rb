# frozen_string_literal: true

RSpec.describe "Lowercase Tokenizer" do
  before do
    TokenKit.configure do |config|
      config.strategy = :lowercase
    end
  end

  after { TokenKit.reset }

  it "splits on non-letters and lowercases" do
    tokens = TokenKit.tokenize("HELLO-WORLD")
    expect(tokens).to eq(["hello", "world"])
  end

  it "always lowercases regardless of config" do
    expect {
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.lowercase = false  # This should be ignored
      end
    }.to output(/Warning: The :lowercase strategy always lowercases text/).to_stderr

    tokens = TokenKit.tokenize("TEST")
    expect(tokens).to eq(["test"])
  end

  it "splits on numbers" do
    tokens = TokenKit.tokenize("TEST123DONE")
    expect(tokens).to eq(["test", "done"])
  end

  it "splits on punctuation" do
    tokens = TokenKit.tokenize("HELLO, WORLD!")
    expect(tokens).to eq(["hello", "world"])
  end

  it "splits on spaces" do
    tokens = TokenKit.tokenize("HELLO WORLD")
    expect(tokens).to eq(["hello", "world"])
  end

  it "splits on special characters" do
    tokens = TokenKit.tokenize("USER@EXAMPLE.COM")
    expect(tokens).to eq(["user", "example", "com"])
  end

  it "handles multiple consecutive non-letters" do
    tokens = TokenKit.tokenize("HELLO---WORLD")
    expect(tokens).to eq(["hello", "world"])
  end

  it "returns empty array for empty string" do
    tokens = TokenKit.tokenize("")
    expect(tokens).to eq([])
  end

  it "returns empty array for string with no letters" do
    tokens = TokenKit.tokenize("123!@#")
    expect(tokens).to eq([])
  end

  it "lowercases unicode letters" do
    tokens = TokenKit.tokenize("CAF√â-NA√èVE")
    expect(tokens).to eq(["caf√©", "na√Øve"])
  end

  context "multi-character lowercasing" do
    it "handles Turkish ƒ∞ (capital I with dot above)" do
      # Turkish ƒ∞ (U+0130) lowercases to i (U+0069) + combining dot (U+0307)
      tokens = TokenKit.tokenize("ƒ∞STANBUL")
      expect(tokens).to eq(["iÃástanbul"])
      expect(tokens.first.chars.count).to eq(9) # i + combining + stanbul = 9 chars
    end

    it "handles multi-char lowercasing mixed with regular characters" do
      # ƒ∞ in the middle of a word
      tokens = TokenKit.tokenize("TESTƒ∞NG")
      expect(tokens).to eq(["testiÃáng"])
      expect(tokens.first.chars.count).to eq(8) # t-e-s-t-iÃá-n-g (iÃá is 2 chars)
    end

    it "handles multiple words with multi-char lowercasing" do
      tokens = TokenKit.tokenize("ƒ∞STANBUL ƒ∞ZMƒ∞R")
      expect(tokens).to eq(["iÃástanbul", "iÃázmiÃár"])
      expect(tokens[0].chars.count).to eq(9) # iÃá-s-t-a-n-b-u-l
      expect(tokens[1].chars.count).to eq(7) # iÃá-z-m-iÃá-r (two ƒ∞ characters)
    end

    it "handles multiple ƒ∞ characters in one word" do
      tokens = TokenKit.tokenize("ƒ∞ƒ∞")
      expect(tokens).to eq(["iÃáiÃá"])
      expect(tokens.first.chars.count).to eq(4) # Two iÃá sequences
    end

    it "handles ƒ∞ at different positions" do
      # Start, middle, end
      tokens = TokenKit.tokenize("ƒ∞TALƒ∞A")
      expect(tokens).to eq(["iÃátaliÃáa"])
      expect(tokens.first.chars.count).to eq(8) # iÃá-t-a-l-iÃá-a
    end
  end

  it "handles mixed case input" do
    tokens = TokenKit.tokenize("MiXeD-CaSe")
    expect(tokens).to eq(["mixed", "case"])
  end

  it "handles CJK characters" do
    tokens = TokenKit.tokenize("Êó•Êú¨Ë™û123TEST")
    expect(tokens).to eq(["Êó•Êú¨Ë™û", "test"])
  end

  context "with remove_punctuation option" do
    before do
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.remove_punctuation = true
      end
    end

    it "has no additional effect since punctuation already splits" do
      tokens = TokenKit.tokenize("HELLO!WORLD?")
      expect(tokens).to eq(["hello", "world"])
    end
  end

  context "efficiency over letter + lowercase" do
    it "performs normalization in single pass" do
      # Lowercase tokenizer is more efficient than:
      # 1. Letter tokenizer
      # 2. + lowercase filter
      # Because it does both in one pass
      tokens = TokenKit.tokenize("HELLO123WORLD")
      expect(tokens).to eq(["hello", "world"])
    end
  end

  context "case-insensitive search indexing" do
    it "normalizes for case-insensitive matching" do
      # Index time
      doc_tokens = TokenKit.tokenize("User-Agent: Mozilla/5.0")

      # Query time - user searches with different case
      query_tokens = TokenKit.tokenize("user agent mozilla")

      # Perfect match after normalization
      expect(doc_tokens).to eq(["user", "agent", "mozilla"])
      expect(query_tokens).to eq(["user", "agent", "mozilla"])
    end
  end

  context "product codes and SKUs" do
    it "normalizes product identifiers" do
      tokens = TokenKit.tokenize("SKU-ABC-123")
      expect(tokens).to eq(["sku", "abc"])
    end
  end

  context "cleaning social media text" do
    it "extracts and normalizes words" do
      tokens = TokenKit.tokenize("#TRENDING @USER HTTP://URL.COM")
      expect(tokens).to eq(["trending", "user", "http", "url", "com"])
    end
  end

  context "per-call options" do
    it "can use lowercase strategy in one-off call" do
      tokens = TokenKit.tokenize(
        "TEST123DONE",
        strategy: :lowercase
      )
      expect(tokens).to eq(["test", "done"])
    end

    it "ignores lowercase option when using lowercase strategy" do
      expect {
        tokens = TokenKit.tokenize(
          "TEST",
          strategy: :lowercase,
          lowercase: false  # Should be ignored
        )
        expect(tokens).to eq(["test"])
      }.to output(/Warning: The :lowercase strategy always lowercases text/).to_stderr
    end
  end

  context "comparison with letter tokenizer" do
    it "behaves like letter tokenizer but always lowercase" do
      letter_tokens = TokenKit.tokenize("HELLO-WORLD", strategy: :letter, lowercase: true)
      lowercase_tokens = TokenKit.tokenize("HELLO-WORLD", strategy: :lowercase)

      expect(lowercase_tokens).to eq(letter_tokens)
    end
  end

  context "emoji handling" do
    it "treats emoji as non-alphabetic characters and lowercases" do
      tokens = TokenKit.tokenize("HELLOüî•WORLD", strategy: :lowercase)
      # Emoji should split the text, and text should be lowercased
      expect(tokens).to eq(["hello", "world"])
    end

    it "handles multiple emoji with mixed case" do
      tokens = TokenKit.tokenize("TESTüòÄüòÇDone", strategy: :lowercase)
      expect(tokens).to eq(["test", "done"])
    end

    it "handles emoji with text" do
      tokens = TokenKit.tokenize("I‚ù§Ô∏èRUBY", strategy: :lowercase)
      expect(tokens).to eq(["i", "ruby"])
    end

    it "handles only emoji" do
      tokens = TokenKit.tokenize("üî•üéâüöÄ", strategy: :lowercase)
      expect(tokens).to eq([])
    end
  end

  context "with preserve_patterns" do
    it "preserves gene names while lowercasing others" do
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.preserve_patterns = [/BRCA\d+/, /TP\d+/]
      end

      tokens = TokenKit.tokenize("Patient BRCA1 test TP53 done")
      expect(tokens).to eq(["patient", "BRCA1", "test", "TP53", "done"])
    end

    it "preserves uppercase acronyms" do
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.preserve_patterns = [/[A-Z]{2,}/]
      end

      tokens = TokenKit.tokenize("The FDA and NIH study")
      expect(tokens).to eq(["the", "FDA", "and", "NIH", "study"])
    end

    it "preserves patterns spanning non-letters" do
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.preserve_patterns = [/Anti-CD\d+/]
      end

      tokens = TokenKit.tokenize("Anti-CD3 treatment Anti-CD28")
      expect(tokens).to eq(["Anti-CD3", "treatment", "Anti-CD28"])
    end

    it "preserves measurements" do
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.preserve_patterns = [/\d+(mg|ug|ml)/i]
      end

      tokens = TokenKit.tokenize("DOSE 100mg SAMPLE 50ug")
      expect(tokens).to eq(["dose", "100mg", "sample", "50ug"])
    end

    it "preserves email addresses" do
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.preserve_patterns = [/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/]
      end

      tokens = TokenKit.tokenize("CONTACT John.Doe@example.com NOW")
      expect(tokens).to eq(["contact", "John.Doe@example.com", "now"])
    end

    it "preserves mixed patterns" do
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.preserve_patterns = [
          /USA/,
          /v\d+\.\d+\.\d+/
        ]
      end

      tokens = TokenKit.tokenize("USA VERSION v2.0.1 READY")
      expect(tokens).to eq(["USA", "version", "v2.0.1", "ready"])
    end
  end

end
