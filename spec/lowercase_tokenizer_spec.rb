# frozen_string_literal: true

RSpec.describe "Lowercase Tokenizer" do
  before do
    TokenKit.configure do |config|
      config.strategy = :lowercase
    end
  end

  after { TokenKit.reset }

  it "splits on non-letters and lowercases" do
    tokens = TokenKit.tokenize("HELLO-WORLD")
    expect(tokens).to eq(["hello", "world"])
  end

  it "always lowercases regardless of config" do
    expect {
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.lowercase = false  # This should be ignored
      end
    }.to output(/Warning: The :lowercase strategy always lowercases text/).to_stderr

    tokens = TokenKit.tokenize("TEST")
    expect(tokens).to eq(["test"])
  end

  it "splits on numbers" do
    tokens = TokenKit.tokenize("TEST123DONE")
    expect(tokens).to eq(["test", "done"])
  end

  it "splits on punctuation" do
    tokens = TokenKit.tokenize("HELLO, WORLD!")
    expect(tokens).to eq(["hello", "world"])
  end

  it "splits on spaces" do
    tokens = TokenKit.tokenize("HELLO WORLD")
    expect(tokens).to eq(["hello", "world"])
  end

  it "splits on special characters" do
    tokens = TokenKit.tokenize("USER@EXAMPLE.COM")
    expect(tokens).to eq(["user", "example", "com"])
  end

  it "handles multiple consecutive non-letters" do
    tokens = TokenKit.tokenize("HELLO---WORLD")
    expect(tokens).to eq(["hello", "world"])
  end

  it "returns empty array for empty string" do
    tokens = TokenKit.tokenize("")
    expect(tokens).to eq([])
  end

  it "returns empty array for string with no letters" do
    tokens = TokenKit.tokenize("123!@#")
    expect(tokens).to eq([])
  end

  it "lowercases unicode letters" do
    tokens = TokenKit.tokenize("CAFÃ‰-NAÃVE")
    expect(tokens).to eq(["cafÃ©", "naÃ¯ve"])
  end

  context "multi-character lowercasing" do
    it "handles Turkish Ä° (capital I with dot above)" do
      # Turkish Ä° (U+0130) lowercases to i (U+0069) + combining dot (U+0307)
      tokens = TokenKit.tokenize("Ä°STANBUL")
      expect(tokens).to eq(["iÌ‡stanbul"])
      expect(tokens.first.chars.count).to eq(9) # i + combining + stanbul = 9 chars
    end

    it "handles multi-char lowercasing mixed with regular characters" do
      # Ä° in the middle of a word
      tokens = TokenKit.tokenize("TESTÄ°NG")
      expect(tokens).to eq(["testiÌ‡ng"])
      expect(tokens.first.chars.count).to eq(8) # t-e-s-t-iÌ‡-n-g (iÌ‡ is 2 chars)
    end

    it "handles multiple words with multi-char lowercasing" do
      tokens = TokenKit.tokenize("Ä°STANBUL Ä°ZMÄ°R")
      expect(tokens).to eq(["iÌ‡stanbul", "iÌ‡zmiÌ‡r"])
      expect(tokens[0].chars.count).to eq(9) # iÌ‡-s-t-a-n-b-u-l
      expect(tokens[1].chars.count).to eq(7) # iÌ‡-z-m-iÌ‡-r (two Ä° characters)
    end

    it "handles multiple Ä° characters in one word" do
      tokens = TokenKit.tokenize("Ä°Ä°")
      expect(tokens).to eq(["iÌ‡iÌ‡"])
      expect(tokens.first.chars.count).to eq(4) # Two iÌ‡ sequences
    end

    it "handles Ä° at different positions" do
      # Start, middle, end
      tokens = TokenKit.tokenize("Ä°TALÄ°A")
      expect(tokens).to eq(["iÌ‡taliÌ‡a"])
      expect(tokens.first.chars.count).to eq(8) # iÌ‡-t-a-l-iÌ‡-a
    end
  end

  it "handles mixed case input" do
    tokens = TokenKit.tokenize("MiXeD-CaSe")
    expect(tokens).to eq(["mixed", "case"])
  end

  it "handles CJK characters" do
    tokens = TokenKit.tokenize("æ—¥æœ¬èª123TEST")
    expect(tokens).to eq(["æ—¥æœ¬èª", "test"])
  end

  context "with remove_punctuation option" do
    before do
      TokenKit.configure do |config|
        config.strategy = :lowercase
        config.remove_punctuation = true
      end
    end

    it "has no additional effect since punctuation already splits" do
      tokens = TokenKit.tokenize("HELLO!WORLD?")
      expect(tokens).to eq(["hello", "world"])
    end
  end

  context "efficiency over letter + lowercase" do
    it "performs normalization in single pass" do
      # Lowercase tokenizer is more efficient than:
      # 1. Letter tokenizer
      # 2. + lowercase filter
      # Because it does both in one pass
      tokens = TokenKit.tokenize("HELLO123WORLD")
      expect(tokens).to eq(["hello", "world"])
    end
  end

  context "case-insensitive search indexing" do
    it "normalizes for case-insensitive matching" do
      # Index time
      doc_tokens = TokenKit.tokenize("User-Agent: Mozilla/5.0")

      # Query time - user searches with different case
      query_tokens = TokenKit.tokenize("user agent mozilla")

      # Perfect match after normalization
      expect(doc_tokens).to eq(["user", "agent", "mozilla"])
      expect(query_tokens).to eq(["user", "agent", "mozilla"])
    end
  end

  context "product codes and SKUs" do
    it "normalizes product identifiers" do
      tokens = TokenKit.tokenize("SKU-ABC-123")
      expect(tokens).to eq(["sku", "abc"])
    end
  end

  context "cleaning social media text" do
    it "extracts and normalizes words" do
      tokens = TokenKit.tokenize("#TRENDING @USER HTTP://URL.COM")
      expect(tokens).to eq(["trending", "user", "http", "url", "com"])
    end
  end

  context "per-call options" do
    it "can use lowercase strategy in one-off call" do
      tokens = TokenKit.tokenize(
        "TEST123DONE",
        strategy: :lowercase
      )
      expect(tokens).to eq(["test", "done"])
    end

    it "ignores lowercase option when using lowercase strategy" do
      expect {
        tokens = TokenKit.tokenize(
          "TEST",
          strategy: :lowercase,
          lowercase: false  # Should be ignored
        )
        expect(tokens).to eq(["test"])
      }.to output(/Warning: The :lowercase strategy always lowercases text/).to_stderr
    end
  end

  context "comparison with letter tokenizer" do
    it "behaves like letter tokenizer but always lowercase" do
      letter_tokens = TokenKit.tokenize("HELLO-WORLD", strategy: :letter, lowercase: true)
      lowercase_tokens = TokenKit.tokenize("HELLO-WORLD", strategy: :lowercase)

      expect(lowercase_tokens).to eq(letter_tokens)
    end
  end

  context "emoji handling" do
    it "treats emoji as non-alphabetic characters and lowercases" do
      tokens = TokenKit.tokenize("HELLOğŸ”¥WORLD", strategy: :lowercase)
      # Emoji should split the text, and text should be lowercased
      expect(tokens).to eq(["hello", "world"])
    end

    it "handles multiple emoji with mixed case" do
      tokens = TokenKit.tokenize("TESTğŸ˜€ğŸ˜‚Done", strategy: :lowercase)
      expect(tokens).to eq(["test", "done"])
    end

    it "handles emoji with text" do
      tokens = TokenKit.tokenize("Iâ¤ï¸RUBY", strategy: :lowercase)
      expect(tokens).to eq(["i", "ruby"])
    end

    it "handles only emoji" do
      tokens = TokenKit.tokenize("ğŸ”¥ğŸ‰ğŸš€", strategy: :lowercase)
      expect(tokens).to eq([])
    end
  end

end
